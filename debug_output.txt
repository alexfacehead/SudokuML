Epoch # 0

Remembering experience:
        State: [[0 0 4 3 7 1 2 5 9]
 [3 2 5 8 0 9 7 6 1]
 [9 0 1 2 6 5 0 4 3]
 [4 3 6 1 9 2 5 8 7]
 [1 9 8 6 5 7 0 3 2]
 [0 5 7 4 8 3 9 1 6]
 [6 8 9 7 3 4 1 2 5]
 [7 1 3 5 2 8 6 9 4]
 [5 4 2 9 1 6 0 7 8]]
        Action: (0, 1, 6)
        Reward: -5
        Next state: [[0 6 4 3 7 1 2 5 9]
 [3 2 5 8 0 9 7 6 1]
 [9 0 1 2 6 5 0 4 3]
 [4 3 6 1 9 2 5 8 7]
 [1 9 8 6 5 7 0 3 2]
 [0 5 7 4 8 3 9 1 6]
 [6 8 9 7 3 4 1 2 5]
 [7 1 3 5 2 8 6 9 4]
 [5 4 2 9 1 6 0 7 8]]
        Done: False
        Memory size: 1
        Batch size: 20
state_batch shape: (1, 9, 9, 1)
action_batch shape: (1, 3)
reward_batch shape: (1, 1)
next_state_batch shape: (1, 9, 9, 1)
done_batch shape: (1, 1)
next_q_values shape: (1, 9, 9, 9)
current_q_values shape: (1, 9, 9, 9)
next_q_values_selected shape: (1,)
target_q_values shape (before squeezing): (1, 1)
target_q_values shape (after squeezing): (1,)
target_q_values shape (after expanding dimensions): (1, 1, 1, 1)
mask shape: (1, 9, 9, 9)
target_q_values shape (after applying mask): (1, 9, 9, 9)
target_q_values shape (after reshaping): (1, 729)
Puzzle # 5
Running total step #17
Puzzle step # 0
Chosen action: (3, 3, 6)
Reward: -5
Episode reward: -5

Exploration rate (epsilon): 1.0
Running total step #18
Puzzle step # 0
Chosen action: (5, 7, 5)
Reward: -5
Episode reward: -10

Exploration rate (epsilon): 1.0
Running total step #19
Puzzle step # 0
Chosen action: (7, 1, 4)
Reward: -5
Episode reward: -15

Exploration rate (epsilon): 1.0
Running total step #20
Puzzle step # 0
Chosen action: (6, 5, 4)
Reward: -5
Episode reward: -20

Exploration rate (epsilon): 1.0
Running total step #21
Puzzle step # 0
Chosen action: (5, 8, 4)
Reward: -5
Episode reward: -25

Exploration rate (epsilon): 1.0

Remembering experience:
        State: [[5 3 6 0 7 2 1 9 8]
 [2 1 9 3 8 6 7 5 4]
 [8 4 7 1 9 5 6 2 3]
 [4 7 2 6 3 8 5 1 9]
 [9 5 1 2 4 7 3 8 6]
 [6 0 8 5 1 9 4 7 2]
 [7 9 5 8 6 4 2 3 1]
 [3 0 4 9 0 1 8 6 7]
 [1 8 0 7 2 3 9 4 5]]
        Action: (0, 3, 4)
        Reward: -5
        Next state: [[5 3 6 4 7 2 1 9 8]
 [2 1 9 3 8 6 7 5 4]
 [8 4 7 1 9 5 6 2 3]
 [4 7 2 6 3 8 5 1 9]
 [9 5 1 2 4 7 3 8 6]
 [6 0 8 5 1 9 4 7 2]
 [7 9 5 8 6 4 2 3 1]
 [3 0 4 9 0 1 8 6 7]
 [1 8 0 7 2 3 9 4 5]]
        Done: False
        Memory size: 51
        Puzzle # 10
Running total step #43
Puzzle step # 0
Chosen action: (3, 4, 5)
Reward: -5
Episode reward: -5

Exploration rate (epsilon): 1.0
Running total step #44
Puzzle step # 0
Chosen action: (7, 1, 3)
Reward: -5
Episode reward: -10

Exploration rate (epsilon): 1.0
Running total step #45
Puzzle step # 0
Chosen action: (6, 7, 5)
Reward: -5
Episode reward: -15

Exploration rate (epsilon): 1.0
Running total step #46
Puzzle step # 0
Chosen action: (4, 2, 1)
Reward: -5
Episode reward: -20

Exploration rate (epsilon): 1.0
Running total step #47
Puzzle step # 0
Chosen action: (7, 7, 1)
Reward: -5
Episode reward: -25

Exploration rate (epsilon): 1.0
Batch size: 20
state_batch shape: (20, 9, 9, 1)
action_batch shape: (20, 3)
reward_batch shape: (20, 1)
next_state_batch shape: (20, 9, 9, 1)
done_batch shape: (20, 1)
next_q_values shape: (20, 9, 9, 9)
current_q_values shape: (20, 9, 9, 9)
next_q_values_selected shape: (20,)
target_q_values shape (before squeezing): (20, 1)
target_q_values shape (after squeezing): (20,)
target_q_values shape (after expanding dimensions): (20, 1, 1, 1)
mask shape: (20, 9, 9, 9)
target_q_values shape (after applying mask): (20, 9, 9, 9)
target_q_values shape (after reshaping): (20, 729)
Puzzle # 15
Running total step #67
Puzzle step # 0
Chosen action: (5, 5, 3)
Reward: -5
Episode reward: -5

Exploration rate (epsilon): 0.9950000047683716
Running total step #68
Puzzle step # 0
Chosen action: (1, 4, 8)
Reward: -5
Episode reward: -10

Exploration rate (epsilon): 0.9950000047683716
Running total step #69
Puzzle step # 0
Chosen action: (4, 2, 2)
Reward: -5
Episode reward: -15

Exploration rate (epsilon): 0.9950000047683716
Running total step #70
Puzzle step # 0
Chosen action: (4, 3, 7)
Reward: -5
Episode reward: -20

Exploration rate (epsilon): 0.9950000047683716
Running total step #71
Puzzle step # 0
Chosen action: (7, 3, 4)
Reward: -5
Episode reward: -25

Exploration rate (epsilon): 0.9950000047683716
Running total step #72
Puzzle step # 0
Chosen action: (7, 6, 2)
Reward: -5
Episode reward: -30

Exploration rate (epsilon): 0.9950000047683716

Remembering experience:
        State: [[9 4 7 8 1 2 5 6 3]
 [5 8 0 7 0 4 1 9 0]
 [2 6 1 9 0 0 4 7 8]
 [1 5 6 3 4 9 2 8 7]
 [8 7 9 6 2 1 3 5 4]
 [3 2 4 5 0 7 6 1 9]
 [6 9 0 2 5 3 7 4 1]
 [7 1 2 4 9 6 8 3 5]
 [4 3 5 1 7 8 0 2 6]]
        Action: (8, 6, 9)
        Reward: -5
        Next state: [[9 4 7 8 1 2 5 6 3]
 [5 8 0 7 0 4 1 9 0]
 [2 6 1 9 0 0 4 7 8]
 [1 5 6 3 4 9 2 8 7]
 [8 7 9 6 2 1 3 5 4]
 [3 2 4 5 0 7 6 1 9]
 [6 9 0 2 5 3 7 4 1]
 [7 1 2 4 9 6 8 3 5]
 [4 3 5 1 7 8 9 2 6]]
        Done: False
        Memory size: 101
        